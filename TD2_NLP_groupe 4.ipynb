{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>message_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                    message_content\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"H:/Documents/Cours/ING3/NLP/TD2/SMSSpamCollection\", sep='\\t', header=None, names=['Type', 'message_content'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test= train_test_split(dataset[\"message_content\"], dataset[\"Type\"], test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2954    urgent  your mobile was awarded a      bonus c...\n",
       "209     you please give us connection today itself bef...\n",
       "2078    hey hun onbus goin   meet him  he wants  go ou...\n",
       "Name: message_content, dtype: object"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After cleaning\n",
    "X_train = X_train.str.replace('\\W', ' ')\n",
    "X_train = X_train.str.replace('\\d+', ' ')\n",
    "X_train = X_train.str.lower()\n",
    "X_train = X_train.str.translate(str.maketrans('','',string.punctuation))\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for message in X_train:\n",
    "    for word in message:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency_message = {unique_word: [0] * len(X_train) for unique_word in vocabulary}\n",
    "\n",
    "for index, message in enumerate(X_train):\n",
    "    for word in message:\n",
    "        words_frequency_message[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pokkiri</th>\n",
       "      <th>contacted</th>\n",
       "      <th>vital</th>\n",
       "      <th>ppm</th>\n",
       "      <th>askin</th>\n",
       "      <th>jungle</th>\n",
       "      <th>ag</th>\n",
       "      <th>pale</th>\n",
       "      <th>brekkie</th>\n",
       "      <th>energy</th>\n",
       "      <th>...</th>\n",
       "      <th>theres</th>\n",
       "      <th>posted</th>\n",
       "      <th>dollar</th>\n",
       "      <th>stars</th>\n",
       "      <th>basically</th>\n",
       "      <th>behave</th>\n",
       "      <th>cantdo</th>\n",
       "      <th>catching</th>\n",
       "      <th>checkmate</th>\n",
       "      <th>dungerees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6425 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pokkiri  contacted  vital  ppm  askin  jungle  ag  pale  brekkie  \\\n",
       "2954        0          0      0    0      0       0   0     0        0   \n",
       "209         0          0      0    0      0       0   0     0        0   \n",
       "2078        0          0      0    0      0       0   0     0        0   \n",
       "3085        0          0      0    0      0       0   0     0        0   \n",
       "1824        0          0      0    0      0       0   0     0        0   \n",
       "\n",
       "      energy  ...  theres  posted  dollar  stars  basically  behave  cantdo  \\\n",
       "2954       0  ...       0       0       0      0          0       0       0   \n",
       "209        0  ...       0       0       0      0          0       0       0   \n",
       "2078       0  ...       0       0       0      0          0       0       0   \n",
       "3085       0  ...       0       0       0      0          0       0       0   \n",
       "1824       0  ...       0       0       0      0          0       0       0   \n",
       "\n",
       "      catching  checkmate  dungerees  \n",
       "2954         0          0          0  \n",
       "209          0          0          0  \n",
       "2078         0          0          0  \n",
       "3085         0          0          0  \n",
       "1824         0          0          0  \n",
       "\n",
       "[5 rows x 6425 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_frequency = pd.DataFrame(words_frequency_message, index=X_train.index)\n",
    "words_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_content</th>\n",
       "      <th>Type</th>\n",
       "      <th>pokkiri</th>\n",
       "      <th>contacted</th>\n",
       "      <th>vital</th>\n",
       "      <th>ppm</th>\n",
       "      <th>askin</th>\n",
       "      <th>jungle</th>\n",
       "      <th>ag</th>\n",
       "      <th>pale</th>\n",
       "      <th>...</th>\n",
       "      <th>theres</th>\n",
       "      <th>posted</th>\n",
       "      <th>dollar</th>\n",
       "      <th>stars</th>\n",
       "      <th>basically</th>\n",
       "      <th>behave</th>\n",
       "      <th>cantdo</th>\n",
       "      <th>catching</th>\n",
       "      <th>checkmate</th>\n",
       "      <th>dungerees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2954</td>\n",
       "      <td>[urgent, your, mobile, was, awarded, a, bonus,...</td>\n",
       "      <td>spam</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>[you, please, give, us, connection, today, its...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2078</td>\n",
       "      <td>[hey, hun, onbus, goin, meet, him, he, wants, ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3085</td>\n",
       "      <td>[ok, lor, i, ned, go, toa, payoh, a, while, re...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>[same, as, u, dun, wan, y, u, dun, like, me, a...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6427 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        message_content  Type  pokkiri  \\\n",
       "2954  [urgent, your, mobile, was, awarded, a, bonus,...  spam        0   \n",
       "209   [you, please, give, us, connection, today, its...   ham        0   \n",
       "2078  [hey, hun, onbus, goin, meet, him, he, wants, ...   ham        0   \n",
       "3085  [ok, lor, i, ned, go, toa, payoh, a, while, re...   ham        0   \n",
       "1824  [same, as, u, dun, wan, y, u, dun, like, me, a...   ham        0   \n",
       "\n",
       "      contacted  vital  ppm  askin  jungle  ag  pale  ...  theres  posted  \\\n",
       "2954          0      0    0      0       0   0     0  ...       0       0   \n",
       "209           0      0    0      0       0   0     0  ...       0       0   \n",
       "2078          0      0    0      0       0   0     0  ...       0       0   \n",
       "3085          0      0    0      0       0   0     0  ...       0       0   \n",
       "1824          0      0    0      0       0   0     0  ...       0       0   \n",
       "\n",
       "      dollar  stars  basically  behave  cantdo  catching  checkmate  dungerees  \n",
       "2954       0      0          0       0       0         0          0          0  \n",
       "209        0      0          0       0       0         0          0          0  \n",
       "2078       0      0          0       0       0         0          0          0  \n",
       "3085       0      0          0       0       0         0          0          0  \n",
       "1824       0      0          0       0       0         0          0          0  \n",
       "\n",
       "[5 rows x 6427 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train = pd.concat([X_train, y_train, words_frequency], axis=1)\n",
    "full_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_messages = full_train[full_train['Type'] == 'spam']\n",
    "ham_messages = full_train[full_train['Type'] == 'ham']\n",
    "\n",
    "p_spam = len(spam_messages) / len(full_train)\n",
    "p_ham = len(ham_messages) / len(full_train)\n",
    "\n",
    "n_words_per_spam_message = spam_messages['message_content'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "n_words_per_ham_message = ham_messages['message_content'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham_messages[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs verification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3368</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hey what are you doing. Y no reply pa..</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>ham</td>\n",
       "      <td>I'm always looking for an excuse to be in the ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4216</td>\n",
       "      <td>ham</td>\n",
       "      <td>No dear i was sleeping :-P</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1407</td>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT, IMPORTANT INFORMATION FOR O2 USER. TOD...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1766</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hi this is yijue... It's regarding the 3230 te...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS prediction\n",
       "3368   ham            Hey what are you doing. Y no reply pa..        ham\n",
       "3261   ham  I'm always looking for an excuse to be in the ...        ham\n",
       "4216   ham                         No dear i was sleeping :-P        ham\n",
       "1407  spam  URGENT, IMPORTANT INFORMATION FOR O2 USER. TOD...       spam\n",
       "1766   ham  Hi this is yijue... It's regarding the 3230 te...        ham"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = X_test.apply(classify_test_set)\n",
    "test_set_clean = pd.concat([y_test, X_test, test_set], axis=1)\n",
    "test_set_clean.columns = ['Label', 'SMS', 'prediction']\n",
    "\n",
    "test_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham that was predicted ham (TP) : 1439\n",
      "spam that was predicted spam (TN) : 203\n",
      "ham that was predicted spam (FN) : 8\n",
      "spam that was predicted ham (FP) : 22\n",
      "\n",
      "Total correct predictions : 1447\n",
      "Total incorrect predictions : 225\n",
      "Accuracy : 0.9820574162679426\n",
      "Precision : 0.9849418206707734\n",
      "Recall : 0.9944713199723566\n",
      "F1 score : 0.9896836313617606\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "false_negative = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "for i in test_set_clean.iterrows() :\n",
    "    row = i[1]\n",
    "    if row[\"Label\"] == \"ham\" and row[\"prediction\"] == \"ham\":\n",
    "        true_positive += 1\n",
    "    elif row[\"Label\"] == \"spam\" and row[\"prediction\"] == \"spam\":\n",
    "        true_negative += 1\n",
    "    elif row[\"Label\"] == \"ham\" and row[\"prediction\"] == \"spam\":\n",
    "        false_negative += 1\n",
    "    else :\n",
    "        false_positive +=1\n",
    "print(\"ham that was predicted ham (TP) :\", true_positive)\n",
    "print(\"spam that was predicted spam (TN) :\", true_negative)\n",
    "print(\"ham that was predicted spam (FN) :\", false_negative)\n",
    "print(\"spam that was predicted ham (FP) :\", false_positive, end=\"\\n\\n\")\n",
    "print(\"Total correct predictions :\", true_positive+false_negative)\n",
    "print(\"Total incorrect predictions :\", true_negative+false_positive)\n",
    "print(\"Accuracy :\", (true_positive+true_negative)/len(test_set_clean))\n",
    "print(\"Precision :\", true_positive/(true_positive+false_positive))\n",
    "print(\"Recall :\", true_positive/(true_positive+false_negative))\n",
    "print(\"F1 score :\", (2*true_positive)/(2*true_positive+false_positive+false_negative))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
