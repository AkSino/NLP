{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "import functools\n",
    "import operator\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from time import time\n",
    "stopwords = stopwords.words(\"english\")\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset ( Dialogue Lines of The Simpsons )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"H:\\Downloads\\simpsons_dataset.csv\"\n",
    "dataset = pd.read_csv(data, sep=',')\n",
    "# remove null line\n",
    "dataset = dataset.dropna().reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Dataset with only principal character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>that life is worth living</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>victory party under the slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>mr bergstrom mr bergstrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>do you know where i could find him</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  raw_character_text                         spoken_words\n",
       "0       Lisa Simpson                 wheres mr bergstrom \n",
       "1       Lisa Simpson           that life is worth living \n",
       "2       Bart Simpson       victory party under the slide \n",
       "3       Lisa Simpson           mr bergstrom mr bergstrom \n",
       "4       Lisa Simpson  do you know where i could find him "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = dataset.groupby('raw_character_text').count().reset_index()\n",
    "principal_character = group[group[\"spoken_words\"]>1000].reset_index()\n",
    "principal_character_message = principal_character.raw_character_text.tolist()\n",
    "dataset = dataset.drop(dataset[~dataset.raw_character_text.isin(principal_character.raw_character_text.tolist())].index).reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSentence1(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ww = sentence.split()\n",
    "    #ww = nltk.word_tokenize(sentence)\n",
    "    #ww = [x for x in ww if x not in stopwords]\n",
    "    ww = [x for x in ww if x !=\"\"]\n",
    "    ww = [lemmatizer.lemmatize(x) for x in ww]\n",
    "    if len(ww)>2:\n",
    "        return \" \".join(ww)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def process_text_1(dataset):\n",
    "    ### Removes non-alphabetic characters:\n",
    "\n",
    "    dataset.spoken_words = [re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in dataset['spoken_words']]\n",
    "    dataset.spoken_words= [re.sub(\"[/']\", '', str(row)).lower() for row in dataset.spoken_words]\n",
    "    cleaned = [processSentence1(x) for x in dataset.spoken_words.tolist()]\n",
    "    datasetCleaned = dataset.copy()\n",
    "    datasetCleaned.spoken_words = cleaned\n",
    "    datasetCleaned = datasetCleaned.dropna()\n",
    "    datasetCleaned = datasetCleaned[~datasetCleaned.spoken_words.duplicated()]\n",
    "    wordsDf = pd.DataFrame(cleaned, columns=[\"spoken_words\"]).dropna().drop_duplicates()\n",
    "    print(\"Words count after processing: \"+ str(len(wordsDf)))\n",
    "    return datasetCleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSentence2(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ww = sentence.split()\n",
    "    #ww = nltk.word_tokenize(sentence)\n",
    "    ww = [x for x in ww if x not in stopwords]\n",
    "    ww = [x for x in ww if x !=\"\"]\n",
    "    ww = [lemmatizer.lemmatize(x) for x in ww]\n",
    "    if len(ww)>2:\n",
    "        return \" \".join(ww)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def process_text_2(dataset):\n",
    "    ### Removes non-alphabetic characters:\n",
    "\n",
    "    dataset.spoken_words = [re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in dataset['spoken_words']]\n",
    "    dataset.spoken_words= [re.sub(\"[/']\", '', str(row)).lower() for row in dataset.spoken_words]\n",
    "    cleaned = [processSentence2(x) for x in dataset.spoken_words.tolist()]\n",
    "    datasetCleaned = dataset.copy()\n",
    "    datasetCleaned.spoken_words = cleaned\n",
    "    datasetCleaned = datasetCleaned.dropna()\n",
    "    datasetCleaned = datasetCleaned[~datasetCleaned.spoken_words.duplicated()]\n",
    "    wordsDf = pd.DataFrame(cleaned, columns=[\"spoken_words\"]).dropna().drop_duplicates()\n",
    "    print(\"Words count after processing: \"+ str(len(wordsDf)))\n",
    "    return datasetCleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    data1 = process_text_1(data)\n",
    "    data2 = process_text_2(data)\n",
    "    return(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count after processing: 71649\n",
      "Words count after processing: 61361\n"
     ]
    }
   ],
   "source": [
    "data_preprocess1, data_preprocess2 = preprocessing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 =data_preprocess1.spoken_words\n",
    "y1 = data_preprocess1.raw_character_text\n",
    "data2 = data_preprocess2.spoken_words\n",
    "y2 =data_preprocess2.raw_character_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliting_dataset(data,y):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "    features_train = vectorizer.fit_transform(data)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train, y)\n",
    "    features_train = selector.transform(features_train).toarray()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(features_train, y)\n",
    "    scores = cross_validate(model, features_train, y, cv=5,return_train_score=True)\n",
    "    y_pred = cross_val_predict(model, features_train,y,cv=5)\n",
    "    return scores, y_pred,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24855\n",
      "24396\n"
     ]
    }
   ],
   "source": [
    "CV1 = spliting_dataset(data1,y1)\n",
    "CV2 = spliting_dataset(data2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores 2: {'fit_time': array([1.22906709, 1.1598103 , 1.22727132, 1.11313009, 1.36836457]), 'score_time': array([0.12617493, 0.12497044, 0.13563633, 0.14058757, 0.14788651]), 'test_score': array([0.36869551, 0.35764342, 0.36147327, 0.35878422, 0.36294003]), 'train_score': array([0.36522164, 0.37004217, 0.37136629, 0.37083664, 0.37067367])}\n"
     ]
    }
   ],
   "source": [
    "print (\"Cross-validated scores 2:\", CV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores 1: {'fit_time': array([1.44807363, 1.35563207, 1.37374949, 1.45717859, 1.35418987]), 'score_time': array([0.17552948, 0.15621281, 0.17417836, 0.1900003 , 0.17625594]), 'test_score': array([0.37124913, 0.36099093, 0.36434054, 0.3581298 , 0.36569195]), 'train_score': array([0.36823741, 0.3719709 , 0.37251173, 0.37436103, 0.37185973])}\n"
     ]
    }
   ],
   "source": [
    "print (\"Cross-validated scores 1:\", CV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Documents\\ING3\\NLP\\VENV\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "conf_mat2 = metrics.confusion_matrix(CV2[1],CV2[2])\n",
    "classi2 = metrics.classification_report(CV2[1],CV2[2])\n",
    "conf_mat1 = metrics.confusion_matrix(CV1[1],CV1[2])\n",
    "classi1 = metrics.classification_report(CV1[1],CV1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess 1: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Bart Simpson       0.10      0.48      0.17      2280\n",
      "C. Montgomery Burns       0.02      0.84      0.03        57\n",
      "       Chief Wiggum       0.01      0.89      0.02        19\n",
      "     Grampa Simpson       0.00      0.00      0.00         0\n",
      "      Homer Simpson       0.98      0.35      0.51     66056\n",
      "   Krusty the Clown       0.00      0.00      0.00         0\n",
      "      Lenny Leonard       0.00      0.00      0.00         0\n",
      "       Lisa Simpson       0.07      0.47      0.12      1286\n",
      "      Marge Simpson       0.11      0.67      0.19      1887\n",
      "Milhouse Van Houten       0.00      0.00      0.00         0\n",
      "        Moe Szyslak       0.00      0.50      0.01        18\n",
      "       Ned Flanders       0.01      0.83      0.01        12\n",
      "       Nelson Muntz       0.01      1.00      0.02         9\n",
      "    Seymour Skinner       0.01      0.80      0.02        25\n",
      "\n",
      "           accuracy                           0.36     71649\n",
      "          macro avg       0.09      0.49      0.08     71649\n",
      "       weighted avg       0.91      0.36      0.49     71649\n",
      "\n",
      "\n",
      "Preprocess 2: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Bart Simpson       0.10      0.49      0.17      1897\n",
      "C. Montgomery Burns       0.02      0.91      0.03        46\n",
      "       Chief Wiggum       0.01      0.87      0.02        15\n",
      "     Grampa Simpson       0.00      0.00      0.00         0\n",
      "      Homer Simpson       0.99      0.35      0.51     56927\n",
      "   Krusty the Clown       0.00      0.00      0.00         0\n",
      "      Lenny Leonard       0.00      0.00      0.00         0\n",
      "       Lisa Simpson       0.06      0.49      0.11       955\n",
      "      Marge Simpson       0.11      0.69      0.18      1460\n",
      "Milhouse Van Houten       0.00      0.00      0.00         0\n",
      "        Moe Szyslak       0.00      0.50      0.01        12\n",
      "       Ned Flanders       0.01      0.82      0.01        11\n",
      "       Nelson Muntz       0.01      1.00      0.02         8\n",
      "    Seymour Skinner       0.01      0.73      0.02        30\n",
      "\n",
      "           accuracy                           0.36     61361\n",
      "          macro avg       0.09      0.49      0.08     61361\n",
      "       weighted avg       0.92      0.36      0.49     61361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocess 1: \\n\",classi1)\n",
    "print(\"\\nPreprocess 2: \\n\",classi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Preprocess 1: \n",
      "\n",
      " [[ 1092    12    43    21   187    67    12   456    97    82    30    16\n",
      "     58   107]\n",
      " [    1    48     1     2     2     1     0     1     0     1     0     0\n",
      "      0     0]\n",
      " [    1     0    17     0     0     0     0     0     1     0     0     0\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [ 9115  2602  1550  1497 23005  1401   955  7863  9892  1296  2457  1699\n",
      "    805  1919]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [  323    10     8    17    68    35     3   610    57    61     3    10\n",
      "     21    60]\n",
      " [   85    31    14    37   131    28    34    62  1266    19    34   103\n",
      "      8    35]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    2     0     0     1     2     0     3     0     0     1     9     0\n",
      "      0     0]\n",
      " [    1     0     0     0     1     0     0     0     0     0     0    10\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      9     0]\n",
      " [    0     0     1     0     0     0     0     3     1     0     0     0\n",
      "      0    20]]\n",
      "\n",
      "Confusion Matrix Preprocess 2:\n",
      "\n",
      " [[  921     8    35    17   143    55     5   389    73    68    19    13\n",
      "     48   103]\n",
      " [    0    42     0     2     0     1     0     1     0     0     0     0\n",
      "      0     0]\n",
      " [    1     0    13     0     0     0     0     0     1     0     0     0\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [ 7714  2314  1381  1283 19715  1244   839  6698  8463  1089  2244  1551\n",
      "    674  1718]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [  235     5    10    16    45    30     1   464    37    49     2     6\n",
      "     18    37]\n",
      " [   59    21    11    31    89    29    26    41  1007    13    24    71\n",
      "      8    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [    1     0     0     1     1     0     2     0     0     1     6     0\n",
      "      0     0]\n",
      " [    1     0     0     0     1     0     0     0     0     0     0     9\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      8     0]\n",
      " [    1     0     1     0     1     1     0     2     1     1     0     0\n",
      "      0    22]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConfusion Matrix Preprocess 1: \\n\\n\",conf_mat1)\n",
    "print(\"\\nConfusion Matrix Preprocess 2:\\n\\n\",conf_mat2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
