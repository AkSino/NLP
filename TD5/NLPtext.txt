

We all know that with Machine Learning you can automatically classify text documents or analyze its subjectivity. We've just released a guide that gives a brief introduction to Text Classification. 

It cover the three most used classifiers; Naive Bayes, Maximum Entropy and Support Vector Machines and will give practical examples in the form of the sentiment analysis of book reviews. 

Originally posted here. 
Introduction:

Natural Language Processing (NLP) is a vast area of Computer Science that is concerned with the interaction between Computers and Human Language[1].
Within NLP many tasks are – or can be reformulated as – classification tasks. In classification tasks we are trying to produce a classification function which can give the correlation between a certain ‘feature’ Dand a class C. This Classifier first has to be trained with a training dataset, and then it can be used to actually classify documents. Training means that we have to determine  its model parameters. If the set of training examples is chosen correctly, the Classifier should predict the class probabilities of the actual documents with a similar accuracy (as the training examples).

After construction, such a Classifier could for example tell us that document containing the words “Bose-Einstein condensate” should be categorized as a Physics article, while documents containing the words “Arbitrage” and “Hedging” should be categorized as a Finance article.
Another Classifier could tell us that mails starting with “Dear Customer/Guest/Sir” (instead of your name) and containing words like “Great opportunity” or “one-time offer” can be classified as spam.
Here we can already see two uses of classification models: topic classification and spam filtering. For these purposes a Classifiers work quiet well and perform better than most trained professionals.

A third usage of Classifiers is Sentiment Analysis. Here the purpose is to determine the subjective value of a text-document, i.e. how positive or negative is the content of a text document. Unfortunately, for this purpose these Classifiers fail to achieve the same accuracy. This is due to the subtleties of human language; sarcasm, irony, context interpretation, use of slang, cultural differences and the different ways in which opinion can be expressed (subjective vs comparative, explicit vs implicit).

In this blog I will discuss the theory behind three popular Classifiers (Naive Bayes, Maximum Entropy and Support Vector Machines) in the context of Sentiment Analysis[2]. In the next blog I will apply this gained knowledge to automatically deduce the sentiment of collected Amazon.com book reviews.

The contents of this blog-post is as follows:

    Basic concepts of text classification:
        Tokenization
        Word normalization
        bag-of-words model
        Classifier evaluation
    Naieve Bayesian Classifier
    Maximum Entropy Classifier
    Support Vector Machines
    What to Expect

1. Basic Concepts

Tokenization:
Tokenization is the name given to the process of chopping up sentences into smaller pieces (words or tokens). The segmentation into tokens can be done with decision trees, which contains information to correctly solve the issues you might encounter. Some of these issues you would have to consider are:
1. The choice for the delimiter will for most cases be a whitespace (“We’re going to Barcelona” -> [“We’re”, “going”, “to”, “Barcelona.”]), but what should you do when you come across words with a white space in them (“We’re going to The Hague.”->[“We’re”, “going”,”to”,”The”, “Hague”]).
2. What should you do with punctuation marks? Although many tokenizers are geared towards throwing punctuation away, for Sentiment analysis a lot of valuable information could be deduced from them. ! puts extra emphasis on the negative/positive sentiment of the sentence, while ? can mean uncertainty (no sentiment).
2. “, ‘ , [], () can mean that the words belong together and should be treated as a separate sentence. Same goes for words which are bold,italic, underlined, or inside a link. If you also want to take these last elements into considerating, you should scrape the html code and not just the text.

Word Normalization:
Word Normalization is the reduction of each word to its base/stem form (by chopping of the affixes). While doing this, we should consider the following issues:
1. Capital letters should be normalized to lowercase, unless it occurs in the middle of a sentence; this could indicate the name of a writer, place, brand etc.
2. What should be done with the apostrophe (‘); “George’s phone” should obviously be tokenized as “George” and “phone”, but I’m, we’re, they’re should be translated as I am, we are and they are. To make it even more difficult; it can also be used as a quotation mark.
3. Ambigious words like High-tech, The Hague, P.h.D., USA, U.S.A., US and us.

Bag-of-words:
After the text has been segmented into sentences, each sentence has been segmented into words, the words have been tokenized and normalized, we can make a simple bag-of-words model of the text. In this bag-of-words representation you only take individual words into account and give each word a specific subjectivity score. This subjectivity score can be looked up in a sentiment lexicon[7]. If the total score is negative the text will be classified as negative and if its positive the text will be classified as positive.

Classifier Evaluation:
For determining the accuracy of a single Classifier, or comparing the results of different Classifier, the F-score is usually used. This F-score is given by

F = \frac{2pr}{p+r}

where p is the precision and r is the recall. The precision is the number of correctly classified examples divided by the total number of classified examples. The recall is the number of correctly classified examples divided by the actual number of examples in the training set.
2. Naive Bayes:

Naive Bayes [3] classifiers are studying the classification task from a Statistical point of view. The starting point is that the probability of a class C is given by the posterior probability P(C|D) given a training document D. Here D refers to all of the text in the entire training set. It is given by D = ( d_1, d_2, .., d_n ) , where d_i is the i_{th} attribute (word) of document D.

Using Bayes’ rule, this posterior probability can be rewritten as:

P(C=c_i|D) = \frac{P(D|C=c_i) \cdot P(C=c_i)}{P(D)}

Since the marginal probability P(D) is equal for all classes, it can be disregarded and the equation becomes:

P(C=c_i|D) = P(D|C=c_i) \cdot P(C=c_i)

The document D belongs to the class C which maximizes this probability, so:

C_{NB} = argmax P(D|C) \cdot P(C)

C_{NB} = argmax P(d_1, d_2, .., d_n | C) \cdot P(C)

Assuming conditional independence of the words d_i, this equation simplifies to:

C_{NB} = argmax P(d_1|C) \cdot P(d_2|C) \cdot \cdot \cdot P(d_n|C) \cdot P(C)

C_{NB} = argmax P(C) \cdot \prod_i P(d_i|C)

Here P(d_i | C) is the conditional probability that word i belongs to class C. For the purpose of text classification, this probability can simply be calculated by calculating the frequency of word i in class C relative to the total number of words in class C.

P(d_i | C) = \frac{count(d_i, C)}{\sum_i count(d_i, C)}

We have seen that we need to multiply the class probability with all of the prior-probabilities of the individual words belonging to that class. The question then is, how do we know what the prior-probabilities of the words are?  Here we need to remember that this is a supervised machine learning algorithm: we can estimate the prior-probabilities with a training set with documents that are already labeled with their classes. With this training set we can train the model and obtain values for the prior probabilities. This trained model can then be used for classifying unlabeled documents.

This is relatively easy to understand with an example. Lets say we have counted the number of words in a set of labeled training documents. In this set each text document has been labeled as either Positive, Neutral or as Negative. The result will then look like :

Capture

From this table we can already deduce each of the class probabilites:

P(C_{pos}) = 0.141,

P(C_{neu}) = 0.723,

P(C_{neg}) = 0.141.

If we look at the sentence  “This blog-post is awesome.”, then the probabilities for this sentence belonging to a specific class are:

P(C_{pos}) = 0.141 \cdot 50 / 240 \cdot 10 / 240 \cdot 100 / 240 \cdot 70 / 240 = 1.49 \cdot 10^{-3}
P(C_{neu} = 0.723 \cdot 500 / 1230 \cdot 90 / 1230 \cdot 600 / 1230 \cdot 20 / 1230 = 1.71 \cdot 10^{-4}
P(C_{neg}) = 0.141 \cdot 50 / 240 \cdot 10 / 240 \cdot 100 / 240 \cdot 10 / 240 = 2.12 \cdot 10^{-4}

This sentence can thus be classified in the positive category.
3. Maximum Entropy:

The principle behind Maximum Entropy [4] is that the correct distribution is the one that maximizes the Entropy / uncertainty and still meets the constraints which are set by the ‘evidence’.

Let me explain this a bit more. In Information Theory, the wordEntropy is used as a unit of measure for the unpredictability of the content of information. If you would throw a fair dice, each of the six outcomes have the same probability of occuring (1/6). Therefore you have maximum uncertainty; an entropy of 1. If the dice is weighted you already know one of the six outcomes has a higher probability of occuring and the uncertainty becomes less. If the dice is weighted so much that the outcome is always six, there is zero uncertainty in the outcome and hence the information entropy is also zero.
The same applies to letters in a word (or words in a sentence): if you assume that every letter has the same probability of occuring you have maximum uncertainty in predicting the next letter. But if you know that letters like E, A, O or I have a higher probability of occuring you have less uncertainty.

Knowing this, we can say that complex data has a high entropy, patterns and trends have lower entropy, information you know for a fact to be true has zero entropy (and therefore can be excluded).
The idea behind Maximum Entropy is that you want a model which is as unbiased as possible; events which are not excluded by known constraints should be assigned as much uncertainty as possible, meaning the probability distribution should be as uniform as possible. You are looking for the maximum value of the Entropy. If this is not entirely clear, I recommend you to read through this example.

The mathematical formula for Entropy is given by H(p) = - \sum p(a,b) log p(a,b), so the most likely probability distribution pis the one that maximizes this entropy:

p = argmax H(p)

It can be shown that the probability distribution has an exponential form and hence is given by:

P(c|d) = \frac{1}{Z(d)} exp ( \sum_i \lambda_i f_i(d,c)),

where f_i(d,c) is a feature function, \lambda_i is the weight parameter of the feature function and Z(d) is a normalization factor given by
Z(d) = \sum_c exp ( \sum_i \lambda_i f_i(d,c) ) .

This feature function is an indicator function, which is expresses the expected value of the chosen statistics (words) in the training set. These feature functions can then be taken as constraints for the classification of the actual dataset (by eliminating the probability distributions P(c|d) which do not fit with these constraints).

Usually, the weight parameters are automatically determined by the Improved Iterative Scaling algorithm. This is simply a gradient descentfunction which can be iterated over until it converges to the global maximum. The pseudocode for the this algorithm is as follows:

    Initialize all weight parameters \lambda_i to zero.
    Repeat until convergence:
        calculate the probability distribution P_{\Lambda}(c|d) with the weight parameters filled in.
        for each parameter \lambda_i calculate \Delta \lambda_i. This is the solution to:
        \sum_c P_{\Lambda}(c,d) \cdot f_i(d,c) \cdot exp ( \Delta \lambda_i f^{\#} (d,c) ) = \sum_d f_i(d,c)
        update the value for the weight parameter:
        \lambda_i := \lambda_i + \Delta \lambda_i

In step 2b f^{\#}(d,c) is given by the sum of all features in the training dataset d: f^{\#} (d,c) = \sum_i f_i (d,c)

Maximum Entropy is a general statistical classification algorithm and can be used to estimate any probability distribution. For the specific case of text classification, we can limit its form a bit more by using word counts as features:

f_{w,c'}(d,c) = 0 if c \neq c'
f_{w,c'}(d,c) = \frac{count(d,w)}{count(d)} otherwise.
4. Support Vector Machines:

Although it is not immediatly obvious from the name, the SVM algorithm is a ‘simple’ linear classification/regression algorithm[6]. It tries to find ahyperplane which seperates the data in two classes as optimally as possible.
Here as optimally as possible means that as much points as possible of label A should be seperated to one side of the hyperplane and as points of label B to the other side, while maximizing the distance of each point to this hyperplane.

 

svm_max_sep_hyperplane_with_margin

In the image above we can see this illustrated for the example of points plotted in 2D-space. The set of points are labeled  with two categories (illustrated here with black and white points) and SVM chooses the hypeplane that maximizes the margin between the two classes. This hyperplane is given by

< \vec{w} \cdot \vec{x} > + b = \sum_i y_i \alpha_i < \vec{x_i} \cdot \vec{x}> + b = 0

where \vec{x_i} = (x_{i1}, x_{i2}, .. , x_{in} ) is a n-dimensional input vector, y_i is its output value, \vec{w} = (w_1, w_2, .. , w_n )  is the weight vector (the normal vector) defining the hyperplane and the \alpha_i terms are the Lagrangian multipliers.

Once the hyperplane is constructed (the vector \vec{w} is defined) with a training set, the class of any other input vector \vec{x_i} can be determined:  if\vec{w} \cdot \vec{x_i} + b \ge 0 then it belongs to the positive class (the class we are interested in), otherwise it belongs to the negative class (all of the other classes).

We can already see this leads to two interesting questions:
1. SVM only seems to work when the two classes are linearly separable. How can we deal with non-linear datasets? Here I feel the urge to point out that the Naive Bayes and Maximum Entropy are linear classifiers as well and most text documents will be linear. Our training example of Amazon book reviews will be linear as well. But an explanation of the SVM system will not be complete without an explanation of Kernel functions.
2. SVM only seems to be able to separate the dataset into two classes? How can we deal with datasets with more than two classes. For Sentiment Classification we have for example three classes (positive, neutral, negative) and for Topic Classification we can have even more than that.

Kernel Functions:
The classical SVM system requires that the dataset is linearly separable, i.e. there is a single hyperplane which can separate the two classes. For non-linear datasets a Kernel function is used to map the data to a higher dimensional space in which it is linearly separable. Thisvideo gives a good illustation of such a mapping. In this higher dimensional feature space, the classical SVM system can then be used to construct a hyperplane.

Multiclass classification:
The classical SVM system is a binary classifier, meaning that it can only separate the dataset into  two classes. To deal with datasets with more than two classes usually the dataset is reduced to a binary class dataset with which the SVM can work. There are two approaches for decomposing a multiclass classification problem to a binary classification problem: the one-vs-all and one-vs-one approach.
In the one-vs-all approach one SVM Classifier is build per class. This Classifier takes that one class as the positive class and the rest of the classes as the negative class. A datapoint is then only classified within a specific class if it is accepted by that Class’ Classifier and rejected by all other classifiers. Although this can lead to accurate results (if the dataset is clustered), a lot of datapoints can also be left unclassified (if the dataset is not clustered).
In the one-vs-one approach, you build one SVM Classifier per chosen pair of classes. Since there are 0.5N(N-1) possible pair combinations for a set of N classes, this means you have to construct more Classifiers. Datapoints are then categorized in the class for which they have received the most points.

In our example, there are only three classes (positive, neutral, negative) so there is no real difference between these two approaches. In both approaches we have to construct two hyperplanes; positive vs the rest and negative vs the rest. 
What to expect:

For the purpose of testing these Classification methods, I have collected >300.000 book reviews of 10 different books from Amazon.com. I will use a part of these book reviews for training purposes and a part as the test dataset. In the next few blogs I will try to automatically classify the sentiment of these reviews with the four models described above.

PS: Please feel free to contact me if you see missing or unclear information.

PS2: Dont forget to follow my blog.


[1] Machine Learning Literature:
Foundations of Statistical Natural Language Processing by Manning and Schutze,
Machine Learning: A probabilistic perspective by Kevin P. Murphy,
Foundations of Machine Learning by Mehryar Mohri


[2]Sentiment Analysis literature:
There is already a lot of information available and a lot of research done on Sentiment Analysis.  To get a basic understanding and some background information, you can read Pang et.al.’s 2002 article. In this article, the different Classifiers are explained and compared for sentiment analysis of Movie reviews (IMDB). This research was very close to Turney’s 2002 research on Sentiment Analysis of movie reviews (see article). You can also read Bo Pang and Lillian Lee’s 2009article , which is more general in nature (about the challenges of SA, the different ML techniques  etc.)
There are also two relevant books: Web Data Mining and Sentiment Analysis, both by Bing Liu. And last but not least, works of Socher are also quiet interesting (see paper, website containing live demo); it even has inspired this kaggle competition.


[3] Naive Bayes Literature:
Machine Learning by Tom Mitchel, Stanford’s IR-book, Sebastian Raschka’s blog-post, Stanford’s online NLP course.


[4]Maximum Entropy Literature:
Using Maximum Entropy for text classification (1999), A simple introduction to Maximum Entropy models (1997), A brief MaxEnt tutorial,another good MIT article.


[6]SVM Literature:
This youtube video gives a general idea about SVM. For a more technical explanation, this and  this article can be read. Here you can find a good explanation as well as a list of the mostly used Kernel functions. one-vs-one and one-vs-all.


[7] Sentiment Lexicons:
I have selected a list of sentiment analysis lexicons; most of these were mentioned in the Natural Language Processing course, the rest are from stackoverflow. 

    WordStat sentiment Dictionary; This is probably one of the largest lexicons freely available. It contains ~14.000 words ( 9164 negative and 4847 positive words ) and gives words a binary classification (positive or a negative ) score.
    Bill McDonalds 2014 Master dictionary, containing ~85.000 word
    Harvard Inquirer; Contains about ~11.780 words and has a more complex way of ‘scoring’ words; each word can be scored in 15+ categories; words can be Positiv-Negative, Strong-Weak, Active-Passive, Pleasure-Pain, words can indicate pleasure, pain, virtue and vice etc etc
    SentiWordNet; gives the words a positive or negative score between 0 and 1. It contains about 117.660 words, however only ~29.000 of these words have been scored (either positive or negative).
    MPQA; contains about ~8.200 words and binary classifies each word (as either positive or as negative). It also gives additional information such as whether a word is an adjective or a noun and whether a word is ‘strong subjective’ or ‘weak subjective’.
    Bing Liu’s opinion lexicon; contains 4.782 negative and 2.005 positive words.

Including Emoticons in your dictionary;
None of the dictionaries described above contain emoticons, which might be an essential part of text if you are analyzing social media. So how can we include emoticons in our subjectivity analysis? Everybody knows :) is a positive and :( is a negative emoticon but what exactly does :-| mean and how is it different from :-/?

There are a few emoticon sentiment dictionaries on the web which you could use; Emoticon Sentiment Lexicon created by Hogenboom et. al., containing a list of  477 emoticons which are scored either 1 (positive), 0 (neutral) or -1 (negative). You could also make your own emoticon sentiment dictionary by giving the emoticons the same score as their meaning in words.


Artificial Intelligence Overview

 

AI refers to ‘Artificial Intelligence’ which means making machines capable of performing intelligent tasks like human beings. AI performs automated tasks using intelligence.

 

The term Artificial Intelligence has two key components -

    Automation  
    Intelligence

 

For More Information you visit Original Blog
Goals of Artificial Intelligence

 

Key Goals of Artificial Intelligence

 
Stages of Artificial Intelligence

 

Stage 1 - Machine Learning - It is a set of algorithms used by intelligent systems to learn from experience.

 

Stage 2 - Machine Intelligence - These are the advanced set of algorithms used by machines to learn from experience. Eg - Deep Neural Networks.

 

ArtificiaI Intelligence technology is currently at this stage.

 

Stage 3 - Machine Consciousness - It is self-learning from experience without the need of external data.

 

Different Stages of Artificial Intelligence

 
Types of Artificial Intelligence

 

ANI - Artificial Narrow Intelligence - It comprises of basic/role tasks such as those performed by chatbots, personal assistants like SIRI by Apple and Alexa by Amazon.

 

AGI - Artificial General Intelligence - Artificial General Intelligence comprises of human-level tasks such as performed by self-driving cars by Uber, Autopilot by Tesla. It involves continual learning by the machines.

 

ASI - Artificial Super Intelligence - Artificial Super Intelligence refers to intelligence way smarter than humans.

 
What Makes System AI Enabled

 

AI Enabled Systems

 
Difference Between NLP, AI, ML, DL & NN

 

AI or Artificial Intelligence - Building systems that can do intelligent things.

 

NLP or Natural Language Processing - Building systems that can understand language. It is a subset of Artificial Intelligence.

 

ML or Machine Learning - Building systems that can learn from experience. It is also a subset of Artificial Intelligence.

 

NN or Neural Network - Biologically inspired network of Artificial Neurons.

 

DL or Deep Learning - Building systems that use Deep Neural Network on a large set of data. It is a subset of Machine Learning.

 

Difference Between NLP AI ML DL NN

 
What is Natural Language Processing?

 

Natural Language Processing (NLP) is “ability of machines to understand and interpret human language the way it is written or spoken”.

 

The objective of NLP is to make computer/machines as intelligent as human beings in understanding language.

 

What Is NLP

 

The ultimate goal of NLP is to the fill the gap how the humans communicate(natural language) and what the computer understands(machine language).

 

There are three different levels of linguistic analysis done before performing NLP -

 

Syntax - What part of given text is grammatically true.

Semantics - What is the meaning of given text?

Pragmatics - What is the purpose of the text?

 

NLP deal with different aspects of language such as

 

Phonology - It is systematic organization of sounds in language.

 

Morphology - It is a study of words formation and their relationship with each other.

 

Approaches of NLP for understanding semantic analysis

 

    Distributional - It employs large-scale statistical tactics of Machine Learning and Deep Learning.

    Frame - Based - The sentences which are syntactically different but semantically same are represented inside data structure (frame) for the stereotyped situation.

    Theoretical - This approach is based on the idea that sentences refer to the real word (the sky is blue) and parts of the sentence can be combined to represent whole meaning.

    Interactive Learning - It involves pragmatic approach and user is responsible for teaching the computer to learn the language step by step in an interactive learning environment. 

 

The true success of NLP lies in the fact that humans deceive into believing that they are talking to humans instead of computers.

 
Why Do We Need NLP?

 

With NLP, it is possible to perform certain tasks like Automated Speech and Automated Text Writing in less time.

 

Due to the presence of large data (text) around, why not we use the computers untiring willingness and ability to run several algorithms to perform tasks in no time.

 

These tasks include other NLP applications like Automatic Summarization (to generate summary of given text) and Machine Translation (translation of one language into another)

 
Process of NLP

 

In case the text is composed of speech, speech-to-text conversion is performed.

 

The mechanism of Natural Language Processing involves two processes:

 

    Natural Language Understanding

    Natural Language Generation

 
Natural Language Understanding

 

NLU or Natural Language Understanding tries to understand the meaning of given text. The nature and structure of each word inside text must be understood for NLU. For understanding structure, NLU tries to resolve following ambiguity present in natural language:

 

    Lexical Ambiguity - Words have multiple meanings

    Syntactic Ambiguity - Sentence having multiple parse trees.

    Semantic Ambiguity - Sentence having multiple meanings

    Anaphoric Ambiguity - Phrase or word which is previously mentioned but has a different meaning.

 

Next, the meaning of each word is understood by using lexicons (vocabulary) and set of grammatical rules.

 

However, there are certain different words having similar meaning (synonyms) and words having more than one meaning (polysemy).

 
Natural Language Generation

 

It is the process of automatically producing text from structured data in a readable format with meaningful phrases and sentences. The problem of natural language generation is hard to deal with. It is subset of NLP

 

Natural language generation divided into three proposed stages:-

 

1. Text Planning - Ordering of the basic content in structured data is done.

2. Sentence Planning - The sentences are combined from structured data to represent the flow of information.

3. Realization - Grammatically correct sentences are produced finally to represent text.

 
Difference Between NLP and Text Mining or Text Analytics

 

Natural language processing is responsible for understanding meaning and structure of given text.

 

Text Mining or Text Analytics is a process of extracting hidden information inside text data through pattern recognition.

 

Difference Between NLP Text Mining

 

Natural language processing is used to understand the meaning (semantics) of given text data, while text mining is used to understand structure (syntax) of given text data.

 

As an example - I found my wallet near the bank. The task of NLP is to understand in the end that ‘bank’ refers to financial institute or ‘river bank'.

 
What is Big Data?

 

According to the Author Dr. Kirk Borne, Principal Data Scientist, Big Data Definition is described as big data is everything, quantified, and tracked.

  
NLP for Big Data is the Next Big Thing

 

Today around 80 % of total data is available in the raw form. Big Data comes from information stored in big organizations as well as enterprises. Examples include information of employees, company purchase, sale records, business transactions, the previous record of organizations, social media etc.

 

Though human uses language, which is ambiguous and unstructured to be interpreted by computers, yet with the help of NLP, this huge unstructured data can be harnessed for evolving patterns inside data to better know the information contained in data.

 

NLP can solve big problems of the business world by using Big Data. Be it any business like retail, healthcare, business, financial institutions.

 
What is Chatbot?

 
Chatbots or Automated Intelligent Agents

 

    These are the computer program you can talk to through messaging apps, chat windows or through voice calling apps.

    These are intelligent digital assistants used to resolve customer queries in a cost-effective, quick, and consistent manner.

 

Importance of Chatbots

 

Chatbots are important to understanding changes in digital customer care services provided and in many routine queries that are most frequently enquired.

 

Chatbots are useful in a certain scenario when the customer service requests are specific in the area and highly predictable, managing a high volume of similar requests, automated responses.

 
Working of Chatbot

 

What is Chatbot

Image Source - blog.wizeline.com

 

Knowledge Base - It contains the database of information that is used to equip chatbots with the information needed to respond to queries of customers request.

 

Data Store - It contains interaction history of chatbot with users.

 

NLP Layer - It translates users queries (free form) into information that can be used for appropriate responses.

 

Application Layer - It is the application interface that is used to interact with the user.

 

Chatbots learn each time they make interaction with the user trying to match the user queries with the information in the knowledge base using machine learning.

 
Why Deep Learning Needed in NLP

 

    It uses a rule-based approach that represents Words as ‘One-Hot’ encoded vectors.

    Traditional method focuses on syntactic representation instead of semantic representation.

    Bag of words - classification model is unable to distinguish certain contexts.

 

Machine Learning Approach

 
Three Capabilities of Deep Learning

 

Expressibility - This quality describes how well a machine can approximate universal functions.

 

Trainability - How well and quickly a DL system can learn its problem.

 

Generalizability - How well the machine can perform predictions on data that it has not been trained on.

 

There are of course other capabilities that also need to be considered in Deep Learning such as Interpretability, modularity, transferability, latency, adversarial stability, and security. But these are the main ones.

 
Common Tasks of Deep Learning in NLP

 

Deep Learning Algorithms
	

NLP Usage

Neural Network - NN (feed)
	

 

    Part-of-speech Tagging

    Tokenization

    Named Entity Recognition

    Intent Extraction

Recurrent Neural Networks -(RNN)
	

 

    Machine Translation

    Question Answering System

    Image Captioning

Recursive Neural Networks
	

 

    Parsing sentences

    Sentiment Analysis

    Paraphrase detection

    Relation Classification

    Object detection

Convolutional Neural Network -(CNN)
	

 

    Sentence/ Text classification

    Relation extraction and classification

    Spam detection

    Categorization of search queries

    Semantic relation extraction

 

 
Difference Between Classical NLP & Deep Learning NLP

 

 

Difference Between Classical NLP and Deep Learning

Image Source - blog.aylien.com

 
NLP For Log Analysis and Log Mining

 
What is Log? A collection of messages from different network devices and hardware in time sequence represents a log. Logs may be directed to files present on hard disks or can be sent over the network as a stream of messages to log collector. Logs provide the process to maintain and track the hardware performance, parameters tuning, emergency and recovery of systems and optimization of applications and infrastructure.


What is Log Analysis? Log analysis is the process of extracting information from logs considering the different syntax and semantics of messages in the log files and interpreting the context with application to have a comparative analysis of log files coming from different sources for Anomaly Detection and finding correlations.

 
What is Log Mining? Log mining or log knowledge discovery is the process of extracting patterns and correlations in logs to reveal knowledge and predict anomaly detection if any inside log messages.

 

 
Techniques Used for Log Analysis and Log Mining

 

Different techniques used for performing log analysis are described below

 

    Pattern recognition - It is one such technique which involves comparing log messages with messages stored in pattern book to filter out messages.

 

    Normalization -  Normalization of log messages is done to convert different messages into the same format. This is done when different log messages having different terminology but same interpretation is coming from different sources like applications or operating systems.

 

    Classification & Tagging - Classification & Tagging of different log messages involves ordering of messages and tagging them with different keywords for later analysis.

 

    Artificial Ignorance - It is a kind of technique using machine learning algorithms to discard uninteresting log messages. It is also used to detect an anomaly in the normal working of systems.

 
Role of NLP in Log Analysis & Log Mining

 

Natural Language processing techniques are widely used in log analysis and log mining.

 

The different techniques such as tokenization, stemming, lemmatization, parsing etc are used to convert log messages into structured form.

 

Once logs are available in the well-documented form, log analysis, and log mining is performed to extract useful information and knowledge is discovered from information.

 

The example in case of error log caused due to server failure.

 
Diving into Natural Language Processing

 

Natural language processing is a complex field and is the intersection of artificial intelligence, computational linguistics, and computer science.

 
Getting started with NLP 

 

The user needs to import a file containing text written. Then the user should perform the following steps for natural language processing.

 

Technique
	

Example
	

Output

Sentence Segmentation
	

Mark met the president. He said:”Hi! What’s up -Alex?”
	

    Sentence 1 - Mark met the president.

    Sentence 2 - He said: ”Hi! What’s up - Alex?”

Tokenization
	

My phone tries to ‘charging’ from ‘discharging’ state.
	

    [My] [phone] [tries] [to] [‘] [charging] [‘][from] [‘][discharging] [‘] [state][.]

Stemming/Lemmatization
	

Drinking, Drank, Drunk
	

    Drink

Part-of-Speech tagging
	

If you build it he will come.
	

    IN - prepositions and subordinating conjunctions.

    PRP - Personal Pronoun

    VBP - Verb Noun 3rd person singular present form.

    PRP- Personal pronoun

    MD - Modal Verbs

    VB - Verb base form

Parsing
	

Mark and Joe went into a bar.
	

    (S(NP(NP Mark) and (NP(Joe))

    (VP(went (PP into (NP a bar))))

Named Entity Recognition
	

Let’s meet Alice at 6 am in India.
	

    Let’s meet Alice at 6 am in India

    Person Time Location

Coreference resolution
	

Mark went into the mall. He thought it was a shopping mall.
	

    Mark went into the mall. He thought it was a shopping mall.

 

    Sentence segmentation - It identifies sentence boundaries in the given text i.e where one sentence ends and where another sentence begins. Sentences are often marked ended with punctuation mark ‘.’

 

    Tokenization - It identifies different words, numbers, and other punctuation symbols.

 

    Stemming - It strips the ending of words like ‘eating’ is reduced to ‘eat.’

 

    Part of speech (POS) tagging - It assigns each word in a sentence its respective part-of-speech tag such as designating word as noun or adverb.

 

    Parsing - It involves dividing given text into different categories. To answer a question like this part of sentence modify another part of the sentence.

 

    Named Entity Recognition - It identifies entities such as persons, location and time within the documents.

 

    Co-Reference resolution - It is about defining the relationship of given word in a sentence with a previous and the next sentence.

 
Further Key Application Areas of NLP

 

Apart from application in Big Data, Log Mining, and Log Analysis it has other major application areas.

 

Although the term ‘NLP’ is not as popular as ‘big data’ ‘machine learning’ but we are using NLP every day.

 

Automatic summarizer - Given the input text, the task is to write a summary of text discarding irrelevant points.

 

Sentimental analysis - It is done on the given text to predict the subject of the text eg: whether the text conveys judgment, opinion or reviews etc.

 

Text classification - It is performed to categorize different journals, news stories according to their domain. Multi-document classification is also possible. A popular example of text classification is spam detection in emails.

 

Based on the style of the writing in the journal, its attribute can be used to detect its author name.

 

Information Extraction - Information extraction is something which proposes email program to automatically add events to the calendar.

Who swears more? Do Twitter users who mention Donald Trump swear more than those who mention Hillary Clinton? Let’s find out by taking a natural language processing approach (or, NLP for short) to analyzing tweets.

This walkthrough will provide a basic introduction to help developers of all background and abilities get started with the NLP microservices available on Algorithmia. We’ll show you how to chain them together to perform light analysis on unstructured text. Unfamiliar with NLP? Our gentle introduction to NLP will help you get started.

We know that getting started with a new platform or developer tool is an investment in time and energy. Sometimes it can be hard to find the information you need in order to start exploring on your own. That’s why we’ve centralized all our information in the Algorithmia Developer Center and API Docs, where users will find helpful hints, code snippets, and getting started guides. These guides are designed to help developers integrate algorithms into applications and projects, learn how to host their trained machine learning models, or build their own algorithms for others to use via an API endpoint.

Now, let’s tackle a project using some algorithms to retrieve content, and analyze it using NLP. What better place to start than Twitter, and analyzing our favorite presidential candidates?
Twitter, Trump, and Profanity: An NLP Approach

First, let’s find the Twitter-related algorithms on Algorithmia. Go to the search bar on top of the navigation and type in “Twitter”:

Search for algorithms.

You’ll get quite a few results, but find the one called Retrieve Tweets with Keyword, and check out the algorithm page where it will tell you such information as the algorithm’s description, pricing, and the permissions set for this algorithm:

Retrieve Tweets With Keyword algorithm

The algorithm description provides information about the input and output data structures expected, as well as the details regarding any other requirements. For instance, Retrieve Tweets with Keyword requires your Twitter API authentication keys.

At the bottom section of every algorithm page we provide the code samples for your input, output, and how to call the algorithm in Python, Rust, Ruby, JavaScript, NodeJS, cURL, CLI, Java, or Scala. If you have questions about the details of using the Algorithmia API check out the API docs.

Alright, let’s get started!

Here’s the overall structure of our project:

+-- profanity_demo
|   +-- data
|       +-- Donald-Trump-OR-Trump.csv
|       +-- Hillary-Clinton-OR-Hillary.csv
|   +-- logs
|       +-- twitter_data_pull.log
|   +-- profanity_analysis.py
|   +-- twitter_pull_data.py

You’ll need a free Algorithmia account to complete this project. Sign up for free and receive an extra 10,000 credits. Overall, the project will consist of processing around 700 tweets or so with emoticons and other special characters stripped out. This means if a tweet only contained URL’s and emoticons then it won’t be analyzed. Once we pull our data from the Twitter API, we’ll clean it up with some regex, remove stop words, and then find our swear words.
Step One: Retrieve Tweets by Keyword

We’ll use the Retrieve Tweets by Keyword algorithm first in order to query tweets from the Twitter Search API:

 
import os
import csv
import sys
import logging
import Algorithmia

# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

logFile = logging.FileHandler(
    'logs/twitter_pull_data.log')
logFile.setLevel(logging.INFO)

# Creating a custom log format for each line in the log file
formatter = logging.Formatter('%(asctime)s : %(levelname)s : %(message)s')
logFile.setFormatter(formatter)
logger.addHandler(logFile)


# Pass in string query as sys.argv
q_input = sys.argv[1]


def pull_tweets():
    input = {
        "query": q_input,
        "numTweets": "700",
        "auth": {
            "app_key": 'your_consumer_key',
            "app_secret": 'your_consumer_secret_key',
            "oauth_token": 'your_access_token',
            "oauth_token_secret": 'your_access_token_secret'
        }
    }
    client = Algorithmia.client('your_algorithmia_api_key')
    algo = client.algo('twitter/RetrieveTweetsWithKeyword/0.1.3')

    tweet_list = [{'user_id': record['user']['id'], 'retweet_count': record['retweet_count'],
                   'text': record['text']} for record in algo.pipe(input).result]
    return tweet_list


def write_data():
    # Write tweet records to csv for later data processing
    data = pull_tweets()
    filename = os.path.join(q_input.replace(' ', '-'))
    try:
        with open('data/{0}.csv'.format(filename), 'w') as f:
            fieldnames = ['user_id', 'retweet_count', 'text']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for record in data:
                writer.writerow(record)

    except Exception as e:
        logger.info(e)

if __name__ == '__main__':
    write_data()

Okay, let’s go over the obvious parts of the code snippet. This algorithm takes a nested dictionary called ‘input’ that contains the keys: ‘query’, ‘numTweets’ and ‘auth’ which is a dictionary itself. The key ‘query’ is set as a global variable called q_input and holds the system argument that is passed when executing the script. In our case it will hold a presidential nominee name. The key ‘numTweets’ is set to the number of tweets you want to extract and the dictionary ‘auth’ holds the Twitter authentication keys and tokens that you got from Twitter.

As you write the pull_tweets() function, pay attention to the line that sets the variable ‘client’ to ‘Algorithmia.client(algorithmia_api_key)’. This is where you pass in your API key that you were assigned when you signed up for an account with Algorithmia. If you don’t recall where to find that it is in the My Profile page in the Credentials section.

Next notice the variable ‘algo.’ This is where we pass in the path to the algorithm we’re using. Each algorithm’s documentation will give you the appropriate path in the code examples section at the bottom of the algorithm page.

And last, the list comprehension ‘tweet_list’ holds our data after looping through the result of the algorithm by passing in our input variable to algo.pipe(input).result.

Now, you simply write your data to a CSV file that is named after your query. Note: if your query is a space separated string, then the script will join the query with a dash.
Step Two: Collecting Data

It’s time to call our script with our query ‘Donald Trump OR Trump’ which will grab tweets with the terms ‘Donald Trump’ or ‘Trump,’ and will then write a file to your data file called ‘Donald-Trump-OR-Trump.csv’.

python twitter_pull_data.py 'Donald Trump OR Trump'

Try running the script again, but this time passing in ‘Hillary Clinton OR Hillary’ as the query.

With both CSV files in our data folder, we can now create a script called profanity_analysis.py
Step Three: Data Preprocessing

In this next script, we’ll first clean up our dirty data, get rid of emoticons, hashtags, RT’s, etc. Then, we’ll explore the English stop words and profanity algorithms.

"""
Analyze twitter data for profanity of presidential candidates.

Dependent on: stop word, profanity detection algorithms and twitter_pull.py
"""

import os
import re
import csv
import sys
import Algorithmia as alg


# Add in your Algorithmia API key
client = alg.client('your_algorithmia_api_key')


def read_data():
    """Create the list of Tweets from your query."""
    try:
        filename = os.path.join(sys.argv[1].replace(' ', '-'))
        with open('data/{0}.csv'.format(filename)) as data_file:
            data_object = csv.DictReader(data_file, delimiter=',')
            text_data = [tweets['text'] for tweets in data_object]
        return text_data
    except IndexError as ie:
        print(
            "Input error - did you remember to pass in your system argument?",
            ie)
        sys.exit(1)
    except FileNotFoundError as fe:
        print("File not found - check your directory and filename", fe)
        sys.exit(1)
    except:
        raise


def process_text():
    """Remove emoticons, numbers etc. and returns list of cleaned tweets."""
    stripped_text = [
        re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)|^rt|http.+?" +
               sys.argv[1].lower(), '',
               tweets.lower()).strip() for tweets in read_data()
    ]
    return stripped_text

Our first step in cleaning up the data was to use some regex to remove emoticons and numbers. Then, we call the Retrieve Stop Words algorithm to further scrub our data, and helps the Profanity algorithm run a little faster since it doesn’t have to parse through all the common English words that provide no value.

def remove_stop_words():
    """Remove stop words in tweets."""
    try:
        algo = client.algo('nlp/RetrieveStopWords/0.1.1')
        # Input is an empty list
        stop_word_list = algo.pipe([])
        # If our word is not in the stop list than we add it to our word list
        clean_text = ' '.join([word for sentence in process_text()
                               for word in sentence.split(' ')
                               if word not in stop_word_list.result])
        return clean_text
    except Exception as e:
        print(e)
        sys.exit(1)

That’s it for cleaning up our tweets!
Step Four: Checking Tweets for Profanity

Now, we’ll check out the Profanity Detection algorithm and discover the swear words in our tweets. This algorithm is based on around 340 words from noswearing.com, which does a basic string match to catch swear words. Check out the Profanity algorithm page to learn more about the details of the algorithm, and how you can customize your word list by adding your own offensive words since fun, new offensive colloquialisms are constantly being added to the English language everyday. Don’t believe us? Just check out Urban Dictionary for some new favorites that have popped up.

The profanity function is fairly straightforward:

def profanity():
    """Return a dictionary of swear words and their frequency."""
    try:
        algo = client.algo('nlp/ProfanityDetection/0.1.2')
        # Pass in the clean list of tweets combined into a single corpus
        result = algo.pipe([remove_stop_words()]).result
        # Total profanity in corpus
        total = sum(result.values())
        print('Resulting swear words and counts: ', result)
        print('total swear words: ', total)
        return {'profanity_counts': result, 'profanity_sum': total}
    except Exception as e:
        print(e)
        sys.exit(1)

if __name__ == '__main__':
    profanity()

You’re simply passing in the list of words that have been cleaned of English stop words. We’ve joined them into a single corpus since we’re interested in the total profanity of all the tweets from our data, rather than the profanity of each tweet. Our function profanity() prints out both the result of the algorithm along with the total swear words. At the time of this writing there were 30 swear words for the query ‘Donald Trump OR Trump’ and ‘Hillary Clinton OR Clinton’ returns 8 swear words. ????

When we pulled our Twitter data, we also grabbed the user_id and the count of retweets. This is useful because you might want to gauge the popularity of a tweet by doing some light analysis in order to find the probability of whether or not a tweet is likely to be more or less popular given the amount of profanity used.

If you want to see this code in its entirety check out our Sample-Apps on GitHub!
Next Steps

Be sure to check out our other NLP algorithms such as social sentiment analysis or LDA (tags). Microservices like AnalyzeTweets combine the previously mentioned algorithms with one that retrieves tweets. This algorithm returns the negative and positive sentiment of each tweet along with the negative and positive LDA of each tweet. There is no shortage of combinations you can create to do either quick exploratory analysis, or add algorithms such as profanity or Nudity Detection to your app to make sure your content is family friendly.


Deep Learning Research Review Week 3: Natural Language Processing

This is the 3rd installment of a new series called Deep Learning Research Review. Every couple weeks or so, I’ll be summarizing and explaining research papers in specific subfields of deep learning. This week focuses on applying deep learning to Natural Language Processing. The last post was Reinforcement Learning and the post before was Generative Adversarial Networks ICYMI
Introduction to Natural Language Processing

Introduction

                Natural language processing (NLP) is all about creating systems that process or “understand” language in order to perform certain tasks. These tasks could include

    Question Answering (What Siri, Alexa, and Cortana do)
    Sentiment Analysis (Determining whether a sentence has a positive or negative connotation)
    Image to Text Mappings (Generating a caption for an input image)
    Machine Translation (Translating a paragraph of text to another language)
    Speech Recognition
    Part of Speech Tagging
    Name Entity Recognition

The traditional approach to NLP involved a lot of domain knowledge of linguistics itself. Understanding terms such as phonemes and morphemes were pretty standard as there are whole linguistic classes dedicated to their study. Let’s look at how traditional NLP would try to understand the following word.

Let’s say our goal is to gather some information about this word (characterize its sentiment, find its definition, etc). Using our domain knowledge of language, we can break up this word into 3 parts.

We understand that the prefix “un” indicates an opposing or opposite idea and we know that “ed” can specify the time period (past tense) of the word. By recognizing the meaning of the stem word “interest”, we can easily deduce the definition and sentiment of the whole word. Seems pretty simple right? However, when you consider all the different prefixes and suffixes in the English language, it would take a very skilled linguist to understand all the possible combinations and meanings.

How Deep Learning Fits In

                Deep learning, at its most basic level, is all about representation learning. With CNNs, we see the composition of different filters that are used to classify objects into categories. Here, we’re going to take a similar approach with creating representations of words through large datasets.

Overview of This Post

                This post will be structured in a way where we’ll go through the basic building blocks of building deep networks for NLP and then go into talking about some applications through recent research papers. It’ll feel normal to not exactly know why we’re using RNNs or why an LSTM is helpful, but hopefully by the end of the research papers, you’ll have a better sense of why deep learning techniques have helped NLP so much. 
Word Vectors

                Since deep learning loves math, we’re going to represent each word as a d-dimensional vector. Let’s use d = 6.

Now let’s think about how to fill in the values. We want the values to be filled in such a way that the vector somehow represents the word and its context, meaning, or semantics. One method is to create a coocurence matrix. Let’s say that we have the following sentence.

From this sentence, we want to create a word vector for each unique word.

A coocurence matrix is a matrix that contains the number of counts of each word appearing next to all the other words in the corpus (or training set). Let’s visualize this matrix.

Extracting the rows from this matrix can give us a simple initialization of our word vectors.

Notice that through this simple matrix, we’re able to gain pretty useful insights. For example, notice that the words ‘love’ and ‘like’ both contain 1’s for their counts with nouns (NLP and dogs). They also have 1’s for the count with “I”, thus indicating that the words must be some sort of verb. With a larger dataset than just one sentence, you can imagine that this similarity will become more clear as ‘like’, ‘love’, and other synonyms will begin to have similar word vectors, because of the fact that they are used in similar contexts.

Now, although this a great starting point, we notice that the dimensionality of each word will increase linearly with the size of the corpus. If we had a million words (not really a lot in NLP standards), we’d have a million by million sized matrix which would be extremely sparse (lots of 0’s). Definitely not the best in terms of storage efficiency. There have been numerous advancements in finding the most optimal ways to represent these word vectors. The most famous of which is Word2Vec.
Word2Vec

                The basic idea behind word vector initialization techniques is that we want to store as much information as we can in this word vector while still keeping the dimensionality at a manageable scale (25 – 1000 dimensions is ideal). Word2Vec operates on the idea that we want to predict the surrounding words of every word.  Let’s take our previous sentence “I love NLP and I like dogs”. We’re going to look at the first 3 words of this sentence. 3 is thus going to be our window size m.

Now, our goal is to take the center word, ‘love’, and predict the words that come before and after it. How do we do this? By maximizing/optimizing a function of course! Formally, our function seeks to maximize the log probability of any context word given the current center word.

Let’s dig deeper into this. The above cost function is basically saying that we’re going to add the log probabilities of ‘I’ and ‘love’ as well as ‘NLP’ and ‘love’ (where ‘love’ is the center word in both cases). The variable T represents the number of training sentences. Let’s look closer at that log probability.

Vc is the word vector of the center word. Every word has two vector representations (Uo and Uw), one for when the word is used as the center word and one for when it’s used as the outer word. The vectors are trained with stochastic gradient descent. This is definitely one of the more confusing equations to understand, so if you’re still having trouble visualizing what’s happening, you can go here and here for additional resources.

One Sentence Summary: Word2Vec seeks to find vector representations of different words by maximizing the log probability of context words given a center word and modifying the vectors through SGD.

(Optional: The authors of the paper then go into more detail about how negative sampling and subsampling of frequent words can be used to get more precise word vectors. )

Arguably, the most interesting contribution of Word2Vec was the appearance of linear relationships between different word vectors. After training, the word vectors seemed to capture different grammatical and semantic concepts.

It’s pretty incredible how these linear relationships could be formed through a simple objective function and optimization technique.

Bonus: Another cool word vector initialization method: GloVe (Combines the ideas of coocurence matrices with Word2Vec)
Recurrent Neural Networks (RNNs)

                Okay, so now that we have our word vectors, let’s see how they fit into recurrent neural networks. RNNs are the go-to for most NLP tasks today. The big advantage of the RNN is that it is able to effectively use data from previous time steps. This is what a small piece of an RNN looks like.

So, at the bottom we have our word vectors (xt, xt-1, xt+1). Each of the vectors has a hidden state vector at that same time step (ht, ht-1, ht+1). Let’s call this one module.

The hidden state in each module of the RNN is a function of both the word vector and the hidden state vector at the previous time step.

If you take a close look at the superscripts, you’ll see that there’s a weight matrix Whx which we’re going to multiply with our input, and there’s a recurrent weight matrix Whh which is multiplied with the hidden state vector at the previous time step. Keep in mind that these recurrent weight matrices are the same across all time steps. This is the key point of RNNs. Thinking about this carefully, it’s very different from a traditional 2 layer NN for example. In that case, we normally have a distinct W matrix for each layer (W1 and W2). Here, the recurrent weight matrix is the same through the network.

To get the output (Yhat) of a particular module, this will be h times WS, which is another weight matrix.

Let’s take a step back now and understand what the advantages of an RNN are. The most distinct difference from a traditional NN is that an RNN takes in a sequence of inputs (words in our case). You can contrast this to a typical CNN where you’d have just a singular image as input. With an RNN, however, the input can be anywhere from a short sentence to a 5 paragraph essay. Additionally, the order of inputs in this sequence can largely affect how the weight matrices and hidden state vectors change during training. The hidden states, after training, will hopefully capture the information from the past (the previous time steps).
Gated Recurrent Units (GRUs)

                Let’s now look at a gated recurrent unit, or GRU. The purpose of this unit is to provide a more complex way of computing our hidden state vectors in RNNs. This approach will allow us to keep information that capture long distance dependencies.  Let’s imagine why long term dependencies would be a problem in the traditional RNN setup. During backpropagation, the error will flow through the RNN, going from the latest time step to the earliest one. If the initial gradient is a small number (say < 0.25), then by the 3rd or 4th module, the gradient will have practically vanished (chain rule multiplies gradients together) and thus the hidden states of the earlier time steps won’t get updated.

In a traditional RNN, the hidden state vector is computed through this formulation.

The GRU provides a different way of computing this hidden state vector h(t). The computation is broken up into 3 components, an update gate, a reset gate, and a new memory container. The two gates are both functions of the input word vector and the hidden state at the previous time step.

The key difference is that different weights are used for each gate. This is indicated by the differing superscripts. The update gate uses Wz and Uz while the reset gate uses Wr and Ur.

Now, the new memory container is computed through the following.

(The open dot indicates a Hadamard product)

Now, if you take a closer look at the formulation, you’ll see that if the reset gate unit is close to 0, then that whole term becomes 0 as well, thus ignoring the information in ht-1 from the previous time steps. In this scenario, the unit is only a function of the new word vector xt.

The final formulation of h(t) is written as

ht is a function of all 3 components: the reset gate, the update gate, and the memory container. The best way to understand this is by visualizing what happens when zt is close to 1 and when it is close to 0. When zt is close to 1, the new hidden state vector ht is mostly dependent on the previous hidden state and we ignore the current memory container because (1-zt) goes to 0. When zt is close to 0, the new hidden state vector ht is mostly dependent on the current memory container and we ignore the previous hidden state. An intuitive way of looking at these 3 components can be summarized through the following.

    Update Gate:
        If zt ~ 1, then ht completely ignores the current word vector and just copies over the previous hidden state (If this doesn’t make sense, look at the ht equation and take note of what happens to the 1 - zt term when zt ~ 1).
        If zt ~ 0, then ht completely ignores the hidden state at the previous time step and is dependent on the new memory container.
        This gate lets the model control how much of the information in the previous hidden state should influence the current hidden state.
    Reset Gate:
        If rt ~ 1, then the memory container keeps the info from the previous hidden state.
        If rt ~ 0, then the memory container ignores the previous hidden state.
        This gate lets the model drop information if that info is irrelevant in the future.
    Memory Container: Dependent on the reset gate.

A common example to illustrate the effectiveness of GRUs is the following. Let’s say you have the following passage.

and the associated question “What is the sum of the 2 numbers?”. Since the middle sentence has absolutely no impact on the question at hand, the reset and update gates will allow the network to “forget” the middle sentence in some sense, and learn that only specific information (numbers in this case) should modify the hidden state.
Long Short Term Memory Units (LSTMs)

                If you’re comfortable with GRUs, then LSTMs won’t be too far of a leap forward. An LSTM is also made up of a series of gates.

Definitely a lot more information to take in. Since this can be thought of as an extension to the idea behind a GRU, I won’t go too far into the analysis, but for a more in depth walkthrough of each gate and each piece of computation, check out Chris Olah’s amazingly well written blog post. It is by far, the most popular tutorial on LSTMs, and will definitely help those of you looking for more intuition as to why and how these units work so well.
Comparing and Contrasting LSTMs and GRUs

                Let’s start off with the similarities. Both of these units have the special function of being able to keep long term dependencies between words in a sequence. Long term dependencies refer to situations where two words or phrases may occur at very different time steps, but the relationship between them is still critical to solving the end goal. LSTMs and GRUs are able to capture these dependencies through gates that can ignore or keep certain information in the sequence. 

The difference between the two units lies in the number of gates that they have (GRU – 2, LSTM – 3). This affects the number of nonlinearities the input passes through and ultimately affects the overall computation. The GRU also doesn’t have the same memory cell (ct) that the LSTM has.
Before Getting Into the Papers

                Just want to make one quick note. There are a couple other deep models that are useful in NLP. Recursive neural networks and CNNs for NLP are sometimes used in practice, but aren’t as prevalent as RNNs, which really are the backbone behind most deep learning NLP systems.

Alright. Now that we have a good understanding of deep learning in relation to NLP, let’s look at some papers. Since there are numerous different problem areas within NLP (from machine translation to question answering), there are a number of papers that we could look into, but here are 3 that I found to be particularly insightful. 2016 had some great advancements in NLP, but let’s first start with one from 2015.
Memory Networks

Introduction

                The first paper, we’re going to talk about is a quite influential publication in the subfield of Question Answering. Authored by Jason Weston, Sumit Chopra, and Antoine Bordes, this paper introduced a class of models called memory networks.

The intuitive idea is that in order to accurately answer a question regarding a piece of text, you need to somehow store the initial information given to you. If I were to ask you the question “What does RNN stand for”, (assuming you’ve read this post fully J) you’ll be able to give me an answer because the information you absorbed by reading the first part of this post was stored somewhere in your memory. You just had to take a few seconds to locate that info and articulate it in words. Now, I have no clue how the brain is able to do that, but the idea of having a storage place for this information still remains.

The memory network described in the paper is unique because it has an associative memory that it is able to read and write to. It’s interesting to note that we don’t have this type of memory with CNNs or with Q-Networks (for reinforcement learning) or with traditional neural nets. This is in part because the task of question answering relies so heavily upon being able to model or keep track of long-term dependencies, such as keeping track of the characters in a story or a timeline of events. With CNNs and Q-Networks, “memory” is sort of built into the weights of the network as it learns different filters or mappings from states to actions. At first look, RNNs and LSTMs could be used, but these typically aren’t able to remember or memorize inputs from the past (which in question answering is quite critical).

Network Architecture

                Okay, so now let’s look at how this network processes the initial text it is given. Like with most machine learning algorithms, the first step is to convert the input into a feature representation. This could entail using word vectors, part of speech labeling, parsing, etc. It’s really just up to the programmer.

The next step would be taking the feature representation I(x) and allowing our memory m to be updated to reflect the new input x we’ve received.

You can consider the memory m to be a sort of an array that is made up of individual memories mi. Each of these individual memories mi can be a function of the memory m as a whole, the feature representation I(x), and/or itself. This function G can be as simple as just storing the whole representation I(x) in the individual memory unit mi.  You can modify this function G to update past memories based on new input.  The 3rd and 4th steps involve reading from memory, based on the question, to get a feature representation o, and then decoding it to output a final answer r.

The function R could be an RNN that is used to convert the feature representations from memory into a readable and accurate answer to the question.

Now, let’s look at step 3 a little closer. We want this O module to output a feature representation that would best match a possible answer to our given question x. Now this question is going to be compared to every single individual memory unit and is going to be “scored” based on how well the memory unit supports the question.

We take the argmax of the scoring function to find the output representation that best supports the question (You can also take multiple of the highest scoring units, doesn’t have to be limited to 1). The scoring function is one that computes the matrix product between different embeddings of the question and the chosen memory unit[s] (check paper for more details). You can think of this as when you multiply the word vectors of two words in order to find their similarity. This output representation o is then fed into an RNN or LSTM or another scoring function which will output a readable answer.

This network is trained in a supervised manner where training data includes the original text, the question, supporting sentences, and the ground truth answer. Here is the objective function.

For those interested, these are some papers that built off of this memory network approach

    End to End Memory Networks (only requires supervision on outputs, not supporting sentences)
    Dynamic Memory Networks
    Dynamic Coattention Networks (Just got released 2 months ago and had the highest test score on Stanford’s Question Answering Dataset at the time)

Tree LSTMs for Sentiment Analysis

Introduction

                The next paper looks into an advancement in Sentiment Analysis, the task of determining whether a phrase has a positive or negative connotation/meaning. More formally, sentiment can be defined as “a view or attitude toward a situation or event”. At the time, LSTMs were the most commonly used units in sentiment analysis networks. Authored by Kai Sheng Tai, Richard Socher, and Christopher Manning, this paper introduces a novel way of chaining together LSTMs in a non-linear structure.  

The motivation behind this non-linear arrangement lies in the notion that natural language exhibits the property that words in sequence become phrases. These phrases, depending on the order of the words, can hold different meanings from their original word components. In order to represent this characteristic, a network of LSTM units must be arranged into a tree structure where different units are affected by their children nodes.

Network Architecture

                One of the differences between a Tree-LSTM and a standard one is that the hidden state of the latter is a function of the current input and the hidden state at the previous time step. However, with a Tree-LSTM, its hidden state is a function of the current input and the hidden states of its child units.

With this new tree-based structure, there are some mathematical changes including child units having forget gates. For those interested in the details, check the paper for more info. What I would like to focus on, however, is understanding why these models work better than a linear LSTM.

With a Tree-LSTM, a single unit is able to incorporate the hidden states of all of its children nodes. This is interesting because a unit is able to value each of its children nodes differently. During training, the network could realize that a specific word (maybe the word “not” or “very” in sentiment analysis) is extremely important to the overall sentiment of the sentence. The ability to value that node higher provides a lot of flexibility to network and could improve performance.
Neural Machine Translation

Introduction

                The last paper we’ll look at today describes an approach to the task of Machine Translation. Authored by Google ML visionaries Jeff Dean, Greg Corrado, Orial Vinyals, and others, this paper introduced a machine translation system that serves as the backbone behind Google’s popular Translate service. The system reduced translation errors by an average of 60% compared to the previous production system Google used.

Traditional approaches to automated translation include variants of phrase-based matching. This approach required large amounts of linguistic domain knowledge and ultimately its design proved to be too brittle and lacked generalization ability. One of the problems with the traditional approach was that it would try to translate the input sentence piece by piece. It turns out the more effective approach (that NMT uses) is to translate the whole sentence at a time, thus allowing for a broader context and a more natural rearrangement of words.

Network Architecture

                The authors in this paper introduce a deep LSTM network that can be trained end to end with 8 encoder and decoder layers.  We can separate the system into 3 components, the encoder RNN, decoder RNN, and attention module. From a high level, the encoder works on the task on transforming the input sentence to vector representation, the decoder produces the output representation, and then the attention module tells the decoder what to focus on during the task of decoding (This is where the idea of utilizing the whole context of the sentence comes in).

The rest of the paper mainly focuses on the challenges associated with deploying such a service at scale. Topics such as amount of computational resources, latency, and high volume deployment are discussed at length.
Conclusion

                With that, we conclude this post on how deep learning can contribute to natural language processing tasks. In my mind, some future goals in the field could be to improve customer service chatbots, perfect machine translation, and hopefully get question answering systems to obtain a deeper understanding of unstructured or lengthy pieces of text (like Wikipedia pages).


Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value extracted from it. In theory, we can understand and even predict human behaviour using that information.

But there is a problem: one person may generate hundreds or thousands of words in a declaration, each sentence with its corresponding complexity. If you want to scale and analyze several hundreds, thousands or millions of people or declarations in a given geography, then the situation is unmanageable.

Data generated from conversations, declarations or even tweets are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate. Nevertheless, thanks to the advances in disciplines like machine learning a big revolution is going on regarding this topic. Nowadays it is no longer about trying to interpret a text or speech based on its keywords (the old fashioned mechanical way), but about understanding the meaning behind those words (the cognitive way). This way it is possible to detect figures of speech like irony, or even perform sentiment analysis.

    Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.

It is a discipline that focuses on the interaction between data science and human language, and is scaling to lots of industries. Today NLP is booming thanks to the huge improvements in the access to data and the increase in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.


Use Cases of NLP

In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.

NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let’s mention some examples:

    NLP enables the recognition and prediction of diseases based on electronic health records and patient’s own speech. This capability is being explored in health conditions that go from cardiovascular diseases to depression and even schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP to extract disease conditions, medications and treatment outcomes from patient notes, clinical trial reports and other electronic health records.
    Organizations can determine what customers are saying about a service or product by identifying and extracting information in sources like social media. This sentiment analysis can provide a lot of information about customers choices and their decision drivers.
    An inventor at IBM developed a cognitive assistant that works like a personalized search engine by learning all about you and then remind you of a name, a song, or anything you can’t remember the moment you need it to.
    Companies like Yahoo and Google filter and classify your emails with NLP by analyzing text in emails that flow through their servers and stopping spam before they even enter your inbox.
    To help identifying fake news, the NLP Group at MIT developed a new system to determine if a source is accurate or politically biased, detecting if a news source can be trusted or not.
    Amazon’s Alexa and Apple’s Siri are examples of intelligent voice driven interfaces that use NLP to respond to vocal prompts and do everything like find a particular shop, tell us the weather forecast, suggest the best route to the office or turn on the lights at home.
    Having an insight into what is happening and what people are talking about can be very valuable to financial traders. NLP is being used to track news, reports, comments about possible mergers between companies, everything can be then incorporated into a trading algorithm to generate massive profits. Remember: buy the rumor, sell the news.
    NLP is also being used in both the search and selection phases of talent recruitment, identifying the skills of potential hires and also spotting prospects before they become active on the job market.
    Powered by IBM Watson NLP technology, LegalMation developed a platform to automate routine litigation tasks and help legal teams save time, drive down costs and shift strategic focus.

NLP is particularly booming in the healthcare industry. This technology is improving care delivery, disease diagnosis and bringing costs down while healthcare organizations are going through a growing adoption of electronic health records. The fact that clinical documentation can be improved means that patients can be better understood and benefited through better healthcare. The goal should be to optimize their experience, and several organizations are already working on this.

Number of publications containing the sentence “natural language processing” in PubMed in the period 1978–2018. As of 2018, PubMed comprised more than 29 million citations for biomedical literature


Companies like Winterlight Labs are making huge improvements in the treatment of Alzheimer’s disease by monitoring cognitive impairment through speech and they can also support clinical trials and studies for a wide range of central nervous system disorders. Following a similar approach, Stanford University developed Woebot, a chatbot therapist with the aim of helping people with anxiety and other disorders.

But serious controversy is around the subject. A couple of years ago Microsoft demonstrated that by analyzing large samples of search engine queries, they could identify internet users who were suffering from pancreatic cancer even before they have received a diagnosis of the disease. How would users react to such diagnosis? And what would happen if you were tested as a false positive? (meaning that you can be diagnosed with the disease even though you don’t have it). This recalls the case of Google Flu Trends which in 2009 was announced as being able to predict influenza but later on vanished due to its low accuracy and inability to meet its projected rates.

NLP may be the key to an effective clinical support in the future, but there are still many challenges to face in the short term.

Basic NLP to impress your non-NLP friends

The main drawbacks we face these days with NLP relate to the fact that language is very tricky. The process of understanding and manipulating language is extremely complex, and for this reason it is common to use different techniques to handle different challenges before binding everything together. Programming languages like Python or R are highly used to perform these techniques, but before diving into code lines (that will be the topic of a different article), it’s important to understand the concepts beneath them. Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms:

Bag of Words

Is a commonly used model that allows you to count all words in a piece of text. Basically it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.

To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles:

    Words are flowing out like endless rain into a paper cup,

    They slither while they pass, they slip away across the universe

Now let’s count the words:

This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”).

To solve this problem, one approach is to rescale the frequency of words by how often they appear in all texts (not just the one we are analyzing) so that the scores for frequent words like “the”, that are also frequent across other texts, get penalized. This approach to scoring is called “Term Frequency — Inverse Document Frequency” (TFIDF), and improves the bag of words by weights. Through TFIDF frequent terms in the text are “rewarded” (like the word “they” in our example), but they also get “punished” if those terms are frequent in other texts we include in the algorithm too. On the contrary, this method highlights and “rewards” unique or rare terms considering all texts. Nevertheless, this approach still has no context nor semantics.

Tokenization

Is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. Following our example, the result of tokenization would be:

Pretty simple, right? Well, although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign phrases (e.g. laissez faire).

Tokenization can remove punctuation too, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.

The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks.

For deeper details on tokenization, you can find a great explanation in this article.

Stop Words Removal

Includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.

Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time.

There is no universal list of stop words. These can be pre-selected or built from scratch. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on. Nevertheless it seems that the general trend over the past time has been to go from the use of large standard stop word lists to the use of no lists at all.

The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. For example, if we are performing a sentiment analysis we might throw our algorithm off track if we remove a stop word like “not”. Under these conditions, you might select a minimal stop word list and add additional terms depending on your specific objective.

Stemming

refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).

    Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).

The problem is that affixes can create or expand new forms of the same word (called inflectional affixes), or even create new words themselves (called derivational affixes). In English, prefixes are always derivational (the affix creates a new word as in the example of the prefix “eco” in the word “ecosystem”), but suffixes can be derivational (the affix creates a new word as in the example of the suffix “ist” in the word “guitarist”) or inflectional (the affix creates a new form of word as in the example of the suffix “er” in the word “faster”).

Ok, so how can we tell the difference and chop the right bit?

A possible approach is to consider a list of common affixes and rules (Python and R languages have different libraries containing affixes and methods) and perform stemming based on them, but of course this approach presents limitations. Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To offset this effect you can edit those predefined methods by adding or removing affixes and rules, but you must consider that you might be improving the performance in one area while producing a degradation in another one. Always look at the whole picture and test your model’s performance.

So if stemming has serious limitations, why do we use it? First of all, it can be used to correct spelling errors from the tokens. Stemmers are simple to use and run very fast (they perform simple operations on a string), and if speed and performance are important in the NLP model, then stemming is certainly the way to go. Remember, we use it with the objective of improving our performance, not as a grammar exercise.

Lemmatization

Has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best” is changed to “good”), hence standardizing words with similar meaning to their root. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words.

    Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.

For example, the words “running”, “runs” and “ran” are all forms of the word “run”, so “run” is the lemma of all the previous words.

Lemmatization also takes into consideration the context of the word in order to solve other problems like disambiguation, which means it can discriminate between identical words that have different meanings depending on the specific context. Think about words like “bat” (which can correspond to the animal or to the metal/wooden club used in baseball) or “bank” (corresponding to the financial institution or to the land alongside a body of water). By providing a part-of-speech parameter to a word ( whether it is a noun, a verb, and so on) it’s possible to define a role for that word in the sentence and remove disambiguation.

As you might already pictured, lemmatization is a much more resource-intensive task than performing a stemming process. At the same time, since it requires more knowledge about the language structure than a stemming approach, it demands more computational power than setting up or adapting a stemming algorithm.

Topic Modeling

Is as a method for uncovering hidden structures in sets of texts or documents. In essence it clusters texts to discover latent topics based on their contents, processing individual words and assigning them values based on their distribution. This technique is based on the assumptions that each document consists of a mixture of topics and that each topic consists of a set of words, which means that if we can spot these hidden topics we can unlock the meaning of our texts.

From the universe of topic modelling techniques, Latent Dirichlet Allocation (LDA) is probably the most commonly used. This relatively new algorithm (invented less than 20 years ago) works as an unsupervised learning method that discovers different topics underlying a collection of documents. In unsupervised learning methods like this one, there is no output variable to guide the learning process and data is explored by algorithms to find patterns. To be more specific, LDA finds groups of related words by:

    Assigning each word to a random topic, where the user defines the number of topics it wishes to uncover. You don’t define the topics themselves (you define just the number of topics) and the algorithm will map all documents to the topics in a way that words in each document are mostly captured by those imaginary topics.
    The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic. These probabilities are calculated multiple times, until the convergence of the algorithm.

Unlike other clustering algorithms like K-means that perform hard clustering (where topics are disjointed), LDA assigns each document to a mixture of topics, which means that each document can be described by one or more topics (e.g. Document 1 is described by 70% of topic A, 20% of topic B and 10% of topic C) and reflect more realistic results.

Topic modeling is extremely useful for classifying texts, building recommender systems (e.g. to recommend you books based on your past readings) or even detecting trends in online publications.

How does the future look like?

At the moment NLP is battling to detect nuances in language meaning, whether due to lack of context, spelling errors or dialectal differences.

On March 2016 Microsoft launched Tay, an Artificial Intelligence (AI) chatbot released on Twitter as a NLP experiment. The idea was that as more users conversed with Tay, the smarter it would get. Well, the result was that after 16 hours Tay had to be removed due to its racist and abusive comments:


Microsoft learnt from its own experience and some months later released Zo, its second generation English-language chatbot that won’t be caught making the same mistakes as its predecessor. Zo uses a combination of innovative approaches to recognize and generate conversation, and other companies are exploring with bots that can remember details specific to an individual conversation.

Although the future looks extremely challenging and full of threats for NLP, the discipline is developing at a very fast pace (probably like never before) and we are likely to reach a level of advancement in the coming years that will make complex applications look possible.
