{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Premier site de questions](https://www.onlineinterviewquestions.com/nlp-interview-questions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list = []\n",
    "a_list = []\n",
    "nb_questions = 27\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.onlineinterviewquestions.com/nlp-interview-questions/\")\n",
    "for i in range(1, nb_questions+1):\n",
    "    id = \"question\" + str(i)\n",
    "    q = driver.find_element_by_id(id).find_element_by_tag_name(\"h3\").text\n",
    "    a = driver.find_element_by_id(id).find_element_by_tag_name(\"p\").text\n",
    "    if a.replace(\" \", \"\") != \"\":\n",
    "        q_list.append(q)\n",
    "        a_list.append(a)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deuxième site de questions](https://www.i2tutorials.com/nlp-interview-question-answers/nlp-interview-questions-part-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_pages = 7\n",
    "q2_list = []\n",
    "a2_list = []\n",
    "for i in range (1, nb_pages+1):\n",
    "    url = 'https://www.i2tutorials.com/nlp-interview-question-answers/nlp-interview-questions-part-' + str(i) + '/'\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    qas = html.findAll('h3')[:5]\n",
    "    for qa in qas:\n",
    "        q2_list.append(qa.text[3:])\n",
    "        a = \"\"\n",
    "        p = qa.find_next_sibling()\n",
    "        while p is not None and p.name != \"h3\":\n",
    "            a+= \" \" + p.text\n",
    "            p = p.find_next_sibling()\n",
    "        a2_list.append(a)\n",
    "a2_list = [a[9:].replace(u'\\xa0', u'') for a in a2_list]\n",
    "q2_list = [q.replace(u'\\xa0', u'') for q in q2_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tot = q_list + q2_list\n",
    "a_tot = a_list + a2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_qa = {\n",
    "    \"question\":q_tot,\n",
    "    \"answer\":a_tot\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = pd.DataFrame(dict_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "answer      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What Is Nlp?</td>\n",
       "      <td>Natural Language Processing or NLP is an autom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What is NLG (Natural language Generation) ?</td>\n",
       "      <td>It’s about generating new text from understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What Is Pragmatic Analysis In Nlp?</td>\n",
       "      <td>Pragmatic Analysis: It deals with outside word...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  \\\n",
       "20                                 What Is Nlp?   \n",
       "39  What is NLG (Natural language Generation) ?   \n",
       "26           What Is Pragmatic Analysis In Nlp?   \n",
       "\n",
       "                                               answer  \n",
       "20  Natural Language Processing or NLP is an autom...  \n",
       "39  It’s about generating new text from understand...  \n",
       "26  Pragmatic Analysis: It deals with outside word...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa.to_excel('qa.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa.to_csv(path_or_buf='qa.csv', index=False)"
<<<<<<< HEAD
=======
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text for bert training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = \"\"\n",
    "for a in a_tot:\n",
    "    a_text+= \" \" + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = ' Natural Language Processing or NLP is an automated way to understand or analyze the natural languages and extract required information from such data by applying machine learning Algorithms. Below are the few major components of NLP. Natural Language Processing can be used for NLP Terminology is based on the following factors: tf–idf or TFIDF stands for term frequency–inverse document frequency. In information retrieval TFIDF is is a numerical statistic that is intended to reflect how important a word is to a document in a collection or in the collection of a set. According to The Stanford Natural Language Processing Group : Lemmatization generally means to do the things properly with the use of vocabulary and morphological analysis of words. In this process, the endings of the words are removed to return the base word, which is also known as Lemma. Natural Language Processing aims to program computers to process large amounts of natural language data. Tokenization in NLP means the method of dividing the text into various tokens. You can think of a token in the form of the word. Just like a word forms into a sentence. It is an important step in NLP to slit the text into minimal units. Latent Semantic Indexing (LSI) also called Latent semantic analysis is a mathematical method that was developed so that the accuracy of retrieving information can be improved. It helps in finding out the hidden(latent) relationship between the words(semantics) by producing a set of various concepts related to the terms of a sentence to improve the information understanding. The technique used for the purpose is called Singular value decomposition. It is generally useful for working on small sets of static documents. Regular expression is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching. It includes the following elements: Named-entity recognition (NER) is the method of extracting information. It arranges and classifies named entity in the unstructured text in different categories like locations, time expressions, organizations, percentages, and monetary values. NER allows the users to properly understand the subject of the text. Difference between NLP and NLU are Difference between NLP and CI(Conversational Interfaces) Differences between AI, Machine Learning, and NLP Masked language modelling is the process in which the output is taken from the corrupted input. This model helps the learners to master the deep representations in downstream tasks. You can predict a word from the other words of the sentence using this model. Pragmatic Analysis: It deals with outside word knowledge, which means knowledge that is external to the documents and/or queries. Pragmatics analysis that focuses on what was described is reinterpreted by what it actually meant, deriving the various aspects of language that require real-world knowledge. Dependency Parsing is also known as Syntactic Parsing. It is the task of recognizing a sentence and assigning a syntactic structure to it. The most widely used syntactic structure is the parse tree which can be generated using some parsing algorithms. These parse trees are useful in various applications like grammar checking or more importantly it plays a critical role in the semantic analysis stage. Pragmatic Ambiguity can be defined as the words which have multiple interpretations. Pragmatic Ambiguity arises when the meaning of words of a sentence is not specific; it concludes different meanings. There are various sentences in which the proper sense is not understood due to the grammar formation of the sentence; this multi interpretation of the sentence gives rise to ambiguity. The word \"perplexed\" means \"puzzled\" or \"confused\", thus Perplexity in general means the inability to tackle something complicated and a problem that is not specified. Therefore, Perplexity in NLP is a way to determine the extent of uncertainty in predicting some text. N-gram in NLP is simply a sequence of n words, and we also conclude the sentences which appeared more frequently, for example, let us consider the progression of these three words: Natural Language Processing or NLP is an automated way to understand or analyze the natural languages and extract required information from such data by applying machine learning Algorithms.  Below are the few major components of NLP. a. Entity extraction: It involves segmenting a sentence to identify and extract entities, such as a person (real or fictional), organization, geographies, events, etc. b. Syntactic analysis: It refers to the proper ordering of words. c. Pragmatic analysis: It is a part of the process of extracting information from text.  Natural Language Processing can be used for 1. Semantic Analysis 2. Automatic summarization 3. Text classification 4. Question Answering  NLP Terminology is based on the following factors: a. Weights and Vectors: TF-IDF, length(TF-IDF, doc), Word Vectors, Google Word Vectors b. Text Structure: Part-Of-Speech Tagging, Head of sentence, Named entities c. Sentiment Analysis: Sentiment Dictionary, Sentiment Entities, Sentiment Features d. Text Classification: It is a Supervised Learning, Train Set, Dev(=Validation) Set, Test Set, Text Features, LDA. e. Machine Reading: Entity Extraction, Entity Linking,dbpedia, FRED (lib) / Pikes.  Tf–idf or TF IDF stands for term frequency–inverse document frequency. Information retrieval TF IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or in the collection of a set.   According to The Stanford Natural Language Processing Group : Part Of Speech Tagger is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc. PoS taggers use an algorithm to label terms in text bodies. These taggers make more complex categories than those defined as basic PoS, with tags such as “noun-plural” or even more complex labels. PoS is taught to school-age children in English grammar, where children perform basic PoS tagging as part of their education.  Pragmatic Analysis: It deals with outside word knowledge, which means knowledge that is external to the documents and/or queries. It focuses on what was described as interpreted by what it actually meant, deriving the various aspects of language that require real-world knowledge.  Dependency Parsing is also known as Syntactic Parsing. Parsing In Nlp is the task of recognizing a sentence and assigning a syntactic structure to it. It is used syntactic structure is the parse tree which can be generated using some parsing algorithms. They are useful in various applications like grammar checking or more importantly it plays a critical role in the semantic analysis stage.   PAC (Probably Approximately Correct) learning is a learning framework that has been introduced to analyze learning algorithms and their statistical efficiency.  They are as follows. 1. Sequence prediction 2. Sequence generation 3. Sequence recognition 4. Sequential decision   Sequence learning is a method of teaching and learning in a logical manner.  The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model.   Bagging is a method in ensemble for improving unstable estimation or classification schemes. Although, boosting method are used sequentially to reduce the bias of the combined model. Both the methods can reduce errors by reducing the variance term.  The difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of examples that is covered with the candidate rule.   Text classification, Text summarization, Name entity reorganization, part of speech tagging, language model building, Machine translation, Spell checking, speech reorganization, character reorganization.  Splitting the sentence into words is called tokenizaation.  a, the , an etc like repeated words in text, that doesn’t give any additional value to context. we can filter those words by using nltk library standard function.  Remove unwanted data from corpus. Like if you are working sentiment analysis, we have to remove ?”! etc.  WordNet is a lexical database for the English language. It provides short definitions and usage examples, also groups English words into sets of synonyms called synsets, , and records a number of relations among these synonym sets or their members.  It’s about generating new text from understanding old data.   It’s about understanding of natural language. How humans are communicating in different scenarios.  It’s a collection of text documents.  It’s about word analysis, unigram means single word, bigram means double words and trigram means tripple word.  A statistical language model is a probability distribution over sequences of words. For instance if a given a sequence say of length m, it assigns a probability to the whole sequence. This model provides context to distinguish between words and phrases that sound similar.  Latent semantic analysis is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms.   Word embedding libraries are as follows; 1. Word2vec 2. Glove 3. Fasttext 4. Genism  Word2vec is a group of related models that are used to produce word embeddings. They are shallow, two-layer neural network models that are trained to reconstruct linguistic contexts of words.  GloVe, coined from Global Vectors, is a model for distributed word representation. They are unsupervised learning algorithm model for obtaining vector representations for words. Hence, this is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.   fastText is a library for learning of word embeddings and text classification created by Facebook’s AI Research lab. This allows to create an unsupervised learning or supervised learning algorithm model for obtaining vector representations for words  It is a production-ready open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython for top performance and scalability.     Text mining, also referred to as text data mining, roughly equivalent to text analytics is the process of deriving high-quality information from text. Whereas the high-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.   Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing.   Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models. – Topic modelingis a type of statisticalmodelingfor discovering the abstract “topics” that occur in a collection of documents. An example fortopic modelis Latent Dirichlet Allocation (LDA), is used to classify text in a document to a particular topic.   It is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. Is also called as term-document matrix.  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NLPtext.txt', encoding=\"utf8\") as file:\n",
    "    long_description = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2220"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q_tot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) What is NLP?'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"what is WordNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert\n",
    "## Load the BertForQuestionAnswering model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2304 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(q, a_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b791a85034a146898ada1db49fb792a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=798011.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a856315b35434858bd37f5bae3fe3d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=760.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Documents\\Anaconda\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837b27ca81e5408e83dd0f1819e30780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=467042463.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = XLNetLMHeadModel.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "encoded = tokenizer.encode_plus(\"Alright, let's do this\" * 500, return_tensors=\"pt\")\n",
    "print(encoded[\"input_ids\"].shape)  # torch.Size([1, 3503])\n",
    "print(model(**encoded)[0].shape)  # torch.Size([1, 3503, 32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special': True} not recognized.\n",
      "Keyword arguments {'add_special': True} not recognized.\n"
     ]
    }
   ],
   "source": [
    "question = q\n",
    "\n",
    "paragraph = a_text\n",
    "            \n",
    "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=True, truncation=True)\n",
    "#, max_length=20000, truncation=True\n",
    "\n",
    "inputs = encoding['input_ids']  #Token embeddings\n",
    "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = torch.argmax(start_scores)\n",
    "\n",
    "end_index = torch.argmax(end_scores)\n",
    "\n",
    "answer = ' '.join(tokens[start_index:end_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_answer = ''\n",
    "\n",
    "for word in answer.split():\n",
    "    \n",
    "    #If it's a subword token\n",
    "    if word[0:2] == '##':\n",
    "        corrected_answer += word[2:]\n",
    "    else:\n",
    "        corrected_answer += ' ' + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [SEP]'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_answer"
>>>>>>> b34373326edb4ae1496d18ca11d6f49ad15eda4c
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
